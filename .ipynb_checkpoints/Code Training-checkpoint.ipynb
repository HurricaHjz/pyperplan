{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8934fa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from scipy import stats\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from pyperplan import planner as pl\n",
    "from pyperplan import search as sc\n",
    "from pyperplan.planner import (\n",
    "    find_domain,\n",
    "    HEURISTICS,\n",
    "    search_plan,\n",
    "    SEARCHES,\n",
    "    validate_solution,\n",
    "    write_solution,\n",
    ")\n",
    "from pyperplan.task import Operator, Task\n",
    "from pyperplan.heuristics.heuristic_base import Heuristic\n",
    "from pyperplan.search.a_star import greedy_best_first_search as gbfs\n",
    "from typing import Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b54a52",
   "metadata": {},
   "source": [
    "# Planning Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5951b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the planning graph\n",
    "class RelaxedGraph(object):\n",
    "    \n",
    "    def __init__(self): # initialise the Graph\n",
    "        self.num_of_levels: int = 0\n",
    "        self.act = {0: None}\n",
    "        self.prop = {}\n",
    "        self.fixed_point = False\n",
    "        \n",
    "            \n",
    "\n",
    "class RelaxedPlanningGraph(object):\n",
    "    \n",
    "    def __init__(self, domain_file: str, problem_file: str):\n",
    "        # initialise the relaxed planning graph\n",
    "        self.task = pl._ground(pl._parse(domain_file, problem_file))\n",
    "        self.graph = None\n",
    "        self.plan = None\n",
    "        self.hff = -1 # not yet generated\n",
    "        self.dom = domain_file\n",
    "        self.prob = problem_file\n",
    "        self.success = False # whether successfully generated or not\n",
    "        \n",
    "    def create(self, max_level, state = None):\n",
    "        # create the planning graph with a initial state specified\n",
    "        # return the level of relaxed graph generated, -1 if reached fixed point, -2 if reached max depth\n",
    "        self.graph = RelaxedGraph()\n",
    "        if state is not None:\n",
    "            self.graph.prop = {0: set(state)}\n",
    "        else:\n",
    "            self.graph.prop = {0: set(self.task.initial_state)}\n",
    "        goal_set = self.task.goals\n",
    " \n",
    "        for level in range(max_level+1):\n",
    "            current_props = self.graph.prop[level]\n",
    "            # if the goal has been satisfied\n",
    "            if Task.goal_reached(self.task, current_props):\n",
    "                self.success = True\n",
    "                return level\n",
    "            \n",
    "            # else expand the relaxed graph\n",
    "            self.graph.act[level+1] = set([op for op in self.task.operators if op.applicable(current_props)])\n",
    "            \n",
    "            next_props = current_props.copy()\n",
    "            for op in self.graph.act[level+1]:\n",
    "                next_props = next_props | op.add_effects\n",
    "            \n",
    "            if len(current_props) == len(next_props):\n",
    "                return -1 # reached fixed point before finding the goal\n",
    "            self.graph.prop[level+1] = next_props\n",
    "        \n",
    "        return -2 #reached max depth\n",
    "        \n",
    "    def hff_plan(self):\n",
    "        # generate a relaxed plan with hff \n",
    "        # first return error if graph not successfully generated\n",
    "        # return hff, -1 if failed\n",
    "        if self.graph is None:\n",
    "            print(\"Graph not yet generated\")\n",
    "            return -1 # graph not yet created\n",
    "        if not self.success:\n",
    "            print(\"Invalid graph\")\n",
    "            return -1 # graph does not reach goal state\n",
    "        \n",
    "        # otherwise start backtrace\n",
    "        # setup g_k\n",
    "        current_goal = set(self.task.goals.copy())\n",
    "        k = len(self.graph.act.keys())-1\n",
    "        self.plan = {}\n",
    "        for i in range(k, 0, -1):\n",
    "            act_set = set()\n",
    "            # select the minimum set of actions that r-satisfied current goal\n",
    "            for a in self.graph.act[i]:\n",
    "                for eff in a.add_effects:\n",
    "                    if eff in current_goal:\n",
    "                        current_goal.remove(eff)\n",
    "                        act_set.add(a)\n",
    "            # update the current goal to be the goals for previous layer\n",
    "            for a in act_set:\n",
    "                current_goal.update(a.preconditions)\n",
    "                \n",
    "            # update the final plan\n",
    "            self.plan[i] = list(act_set)\n",
    "\n",
    "        \n",
    "        if current_goal.issubset(self.graph.prop[0]):\n",
    "            count = 0\n",
    "            for layer in self.plan.values():\n",
    "                count+=len(layer)\n",
    "            self.hff = count\n",
    "            return self.hff\n",
    "        else:\n",
    "            print(\"something went wrong during planning\")\n",
    "            return -1\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c620cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pick-up d)\n",
      "(pick-up b)\n",
      "(pick-up a)\n",
      "(pick-up c)\n",
      "(put-down d)\n",
      "(put-down b)\n",
      "(put-down a)\n",
      "(put-down c)\n",
      "(stack d d)\n",
      "(stack d b)\n",
      "(stack d a)\n",
      "(stack d c)\n",
      "(stack b d)\n",
      "(stack b b)\n",
      "(stack b a)\n",
      "(stack b c)\n",
      "(stack a d)\n",
      "(stack a b)\n",
      "(stack a a)\n",
      "(stack a c)\n",
      "(stack c d)\n",
      "(stack c b)\n",
      "(stack c a)\n",
      "(stack c c)\n",
      "(unstack d d)\n",
      "(unstack d b)\n",
      "(unstack d a)\n",
      "(unstack d c)\n",
      "(unstack b d)\n",
      "(unstack b b)\n",
      "(unstack b a)\n",
      "(unstack b c)\n",
      "(unstack a d)\n",
      "(unstack a b)\n",
      "(unstack a a)\n",
      "(unstack a c)\n",
      "(unstack c d)\n",
      "(unstack c b)\n",
      "(unstack c a)\n",
      "(unstack c c)\n"
     ]
    }
   ],
   "source": [
    "rpl = RelaxedPlanningGraph(\"benchmarks/blocks/domain.pddl\",\"benchmarks/blocks/task03.pddl\")\n",
    "\n",
    "rpl.create(999)\n",
    "\n",
    "graph = rpl.graph\n",
    "\n",
    "for o in rpl.task.operators:\n",
    "    print(o.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ecbcab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "fs = frozenset([6, 7, 8, 9])\n",
    "s = {1, 2, 3, 4, 5}\n",
    "\n",
    "y = {1,2,6}\n",
    "\n",
    "print(s - y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6d502",
   "metadata": {},
   "source": [
    "# Feature Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0d5d0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vec_relaxed(planning_graph, state, max_level):\n",
    "    \"\"\"\n",
    "    Generate the feature vector followed by the paper from the input (problem, state) pair as described by the paper\n",
    "    Please notice that action and operator are referring to the same type Operator within this function\n",
    "    \n",
    "    Inputs\n",
    "    ----------\n",
    "    planning_graph: the relaxed planning graph DAG pi\n",
    "    state: the current state set of facts\n",
    "    max_level: the maximum layer allowed for ff algorithm to do forward expanding\n",
    "    \n",
    "    Outputs\n",
    "    ----------\n",
    "    feature_vec: a vector of length n + 2*n**2 + 3 representing the feature generated from the given (problem, state) pair\n",
    "                 the first n values are single action feature\n",
    "                 the second 2*n**2 are pairwise action feature\n",
    "                 the last 3 are original heuristic value, the number of layers in pi and the number of unsatisfied goals\n",
    "    \"\"\"\n",
    "    def get_name(op_name):\n",
    "        \"\"\"\n",
    "        transfer an operator name from format of \"(act_name v1 v2...)\" to act_name string\n",
    "        \"\"\"\n",
    "        return op_name.split(\" \")[0][1:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get action schema and output list\n",
    "    act_schema = np.array(list(set([get_name(o.name) for o in planning_graph.task.operators]))) # store names of total action schema\n",
    "    n = len(act_schema)  # length of action schema\n",
    "    total_len = n+2*n**2+3\n",
    "    feature_vec = np.zeros(total_len) # return feature vec, first n is linear, second 2*n**2 is pairwise, last 3 is additional feature\n",
    "    \n",
    "#     if state == planning_graph.task.goals:\n",
    "#         return feature_vec # if already the goal\n",
    "    \n",
    "    # create and get the graph\n",
    "    idx = planning_graph.create(max_level, state)\n",
    "    print(idx)\n",
    "    planning_graph.hff_plan()\n",
    "    graph = planning_graph.graph\n",
    "    act_layers = list(graph.act.values()) # list of layers generated, ith value is the list of actions connencting i-1 th states to ith states layer\n",
    "    print(planning_graph.task.goals,state)\n",
    "    # extract linear feature\n",
    "    #-------------------------------------\n",
    "    # ith value indicate the num of occurance for ith action of act_schema in the entire graph \n",
    "    counter = np.zeros(n)\n",
    "    for act_layer in act_layers:\n",
    "        if act_layer is not None:\n",
    "            for a in act_layer:\n",
    "                counter[act_schema == get_name(a.name)] += 1 \n",
    "    feature_vec[0:n] = counter\n",
    "    \n",
    "    # extract pair-wise feature\n",
    "    #-------------------------------------\n",
    "    # each pair a1, a2 is stored in n + [2*(n*a1+a2), 2*(n*a1+a2)+1]\n",
    "    # e.g. when a1 is 1, a2 is 3, n is 5, store in 5 + [2*(8),  2*(8)+1]\n",
    "    \n",
    "    def to_index(n, index_a1, index_a2, adder):\n",
    "        \"\"\"\n",
    "        return corresponding index in the position of the feature vector\n",
    "        adder is either 0 or 1\n",
    "        index_a1, index_a2 refer to move index in act_schema\n",
    "        \"\"\"\n",
    "        return n+2*(n*index_a1+index_a2)+adder\n",
    "    \n",
    "\n",
    "    def append_to_dict(a, pre, eff_pos):\n",
    "        \"\"\"\n",
    "        add action a into the dicitonary pre and eff_pos\n",
    "        \"\"\"\n",
    "        for p in a.preconditions:\n",
    "            current = pre.get(p)\n",
    "            if current is None:\n",
    "                current = [a]\n",
    "            else:\n",
    "                current.append(a)\n",
    "            pre[p] = current\n",
    "            \n",
    "        for p in a.add_effects:\n",
    "            current = eff_pos.get(p)\n",
    "            if current is None:\n",
    "                current = [a]\n",
    "            else:\n",
    "                current.append(a)\n",
    "            eff_pos[p] = current\n",
    "            \n",
    "            return pre, eff_pos\n",
    "\n",
    "\n",
    "    # define dictionary variables for comparison purpose\n",
    "    # each dictionary maps a fact(proposition) to a list of actions\n",
    "    pre = {}\n",
    "    eff_pos = {}\n",
    "    \n",
    "    # add pre and eff into the empty dictionary for the first layer\n",
    "    for a in act_layers[1]:\n",
    "        pre, eff_pos = append_to_dict(a, pre, eff_pos)\n",
    "   \n",
    "    # loop through second to last action layers\n",
    "    for i in range(2,len(act_layers)): \n",
    "        act_layer = act_layers[i]\n",
    "        \n",
    "        # update fecture vec for the entire layer based on all previously visited layers\n",
    "        for a2 in act_layer:\n",
    "            # count for num of occurances, use set to avoid multiple counts\n",
    "            s1 = set() # feature 1 where eff a1 and pre a2 has intersections\n",
    "            s2 = set() # feature 2 where pre a1 and eff a2 has intersections          \n",
    "            for p in a2.preconditions:\n",
    "                current = eff_pos.get(p)\n",
    "                if current is not None:\n",
    "                    for a1 in current:\n",
    "                        s1.add(a1) \n",
    "\n",
    "            for p in a2.add_effects:\n",
    "                current = pre.get(p)\n",
    "                if current is not None:\n",
    "                    for a1 in current:\n",
    "                        s2.add(a1)\n",
    "\n",
    "            # add index to feature_vec based on set generated:\n",
    "            index_a2 = int(np.where(get_name(a2.name) == act_schema)[0])\n",
    "            for a1 in s1:\n",
    "                # update feature 1 for pair (a1, a2)\n",
    "                index_a1 = int(np.where(get_name(a1.name) == act_schema)[0])\n",
    "                feature_vec[to_index(n, index_a1, index_a2,0)]+=1\n",
    "\n",
    "            for a1 in s2:\n",
    "                # update feature 2 for pair (a1, a2)\n",
    "                index_a1 = int(np.where(get_name(a1.name) == act_schema)[0])\n",
    "                feature_vec[to_index(n, index_a1, index_a2,1)]+=1\n",
    "\n",
    "        # update pre and eff_pos for the entire layer\n",
    "        for a2 in act_layer:\n",
    "            pre, eff_pos = append_to_dict(a2, pre, eff_pos)\n",
    "           \n",
    "    # extract final features\n",
    "    #-------------------------------------\n",
    "    # add heuristic value, number of layers and number of unsatisfied goals\n",
    "    # number of layers:\n",
    "    feature_vec[total_len - 3] = len(act_layers)\n",
    "    # heuristic value hFF: (number of total actions in the plan)\n",
    "    feature_vec[total_len - 2] = planning_graph.hff\n",
    "    # unsatisfied goal: (number of propositions within the initial state that's not in the goal)\n",
    "    ns_goals = 0\n",
    "    for fact in state:\n",
    "        if fact not in planning_graph.task.goals:\n",
    "            ns_goals += 1\n",
    "    feature_vec[total_len - 1] = ns_goals\n",
    "    \n",
    "\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa4039bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_operator(action : str, ops):\n",
    "    \"\"\"\n",
    "    find an operator from the planning graph's ground operator lists\n",
    "    \n",
    "    return: the action operator if found\n",
    "    \"\"\"\n",
    "    for op in ops:\n",
    "        if op.name == action:\n",
    "            return op\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_plan(plan_file_path: str):\n",
    "    \"\"\"\n",
    "    read all the lines from a plan file directory, remove the last line containing cost\n",
    "    \n",
    "    return: a list containing the ground truth plan with length equal to total cost\n",
    "    \"\"\"\n",
    "    with open(plan_file_path, \"r\") as f:\n",
    "\n",
    "        # Read the lines of the file into a list of strings\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    return lines[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fc68364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(domain_file_path, task_file_path, plan_file_path, problem_num : int):\n",
    "    \"\"\"\n",
    "    generate the feature vector matrix X together with a cost vector y from the given input\n",
    "    \n",
    "    Inputs\n",
    "    ----------\n",
    "    domain_file_path: the input domain file\n",
    "    task_file_path: the input problem file\n",
    "    plan_file_path: the input log file that store the optimal plan\n",
    "    problem_num: the problem index for this domain\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None, None if no plan can be found (plan has cost 0)\n",
    "    X : array, shape (plan_length-1, n_features)\n",
    "        The input feature vec of states from initial states all the way towards the second-last state (one state before goal state)\n",
    "    Y : array, shape (plan_length-1, 2)\n",
    "        The input cost vector. If it's a 2D array\n",
    "        The first column is the true cost pi optimal (assume unit cost)\n",
    "        The second column is the probelm_num representing the index of this problem\n",
    "    \n",
    "    \"\"\"\n",
    "    # generate plan and get max level\n",
    "    plan_actions = read_plan(plan_file_path)\n",
    "    if len(plan_actions) == 0:\n",
    "        return None, None\n",
    "    max_level = len(plan_actions)+2\n",
    "    \n",
    "    # generate relaxed planning graph\n",
    "    planning_graph = RelaxedPlanningGraph(domain_file_path, task_file_path)\n",
    "    \n",
    "    # define output matrixes\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # loop from the initial state to the second last state\n",
    "    current_state = planning_graph.task.initial_state\n",
    "    current_cost = len(plan_actions)\n",
    "    for i in range(0,len(plan_actions)-1):\n",
    "        X.append(generate_feature_vec_relaxed(planning_graph, current_state, max_level))\n",
    "        y.append(current_cost)\n",
    "        current_action = find_operator(plan_actions[i], planning_graph.task.operators)\n",
    "        current_state = current_action.apply(current_state)\n",
    "        current_cost -=1\n",
    "        \n",
    "    y = np.c_[y, problem_num * np.ones(len(y))]\n",
    "    return np.asarray(X), np.asarray(y)\n",
    "\n",
    "def generate_problem_matrix(domain_file_path, problem_folder_path, log_folder_path, output_path, title):\n",
    "    \"\"\"\n",
    "    Generate the corresponding feature/label matrix from the given inputs\n",
    "    Stores in the format of \"title.npz\" in the output_path\n",
    "    Each npz file contain two attribute: \"feature\" and \"label\"\n",
    "    \n",
    "    Inputs\n",
    "    ----------\n",
    "    domain_file_path: the path to the domain.pddl\n",
    "    problem_folder_path: the path to the problem_folder containing all the task problem.pddl for generating vectors\n",
    "    plan_folder_path: the plan folder that contains all the log files corresponding to each problem task\n",
    "    output_path: the place to store the generated problem matrix\n",
    "    title: the name for the output npz file\n",
    "    \"\"\"\n",
    "    # get the training problem names and initialise parameters\n",
    "    problem_name_list = [f.split('.')[0] for f in os.listdir(problem_folder_path)]\n",
    "    X = None\n",
    "    Y = None\n",
    "    problem_index = 0\n",
    "    \n",
    "    # generate the final vectors in X, Y\n",
    "    for prob in problem_name_list:\n",
    "        problem_path = problem_folder_path + \"/\" + prob + \".pddl\"\n",
    "        plan_path = log_folder_path + \"/\" + prob + \"_1800.out\"\n",
    "        temp_X, temp_Y = generate_training_data(domain_file_path, problem_path, plan_path, problem_index)\n",
    "        if X is None:\n",
    "            X = temp_X\n",
    "            Y = temp_Y\n",
    "        elif temp_X is not None:\n",
    "            X = np.vstack((X, temp_X))\n",
    "            Y = np.vstack((Y, temp_Y))\n",
    "        print(problem_path,X.shape, Y.shape)\n",
    "        problem_index += 1\n",
    "       \n",
    "        \n",
    "        \n",
    "    # save the final training vectors\n",
    "    np.savez(output_path+\"/\"+title+\".npz\", feature = X, label = Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b09e1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X, y = generate_training_data(\"datasets/domain/transport/domain.pddl\", \n",
    "#                               \"datasets/domain/transport/train/instance-1.pddl\", \n",
    "#                               \"datasets/domain/transport/plans/instance-1_1800.out\",\n",
    "#                               0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7454e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "frozenset({'(on b2 b1)', '(on-table b1)', '(on-table b3)'}) frozenset({'(clear b3)', '(handempty)', '(on b3 b2)', '(on b2 b1)', '(on-table b1)'})\n",
      "datasets/domain/blocks/train/blocks3-task01.pddl (1, 39) (1, 2)\n",
      "4\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(on b2 b3)', '(handempty)', '(on b3 b1)', '(clear b2)', '(on-table b1)'})\n",
      "4\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(holding b2)', '(on-table b1)', '(on b3 b1)'})\n",
      "3\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(handempty)', '(on b3 b1)', '(clear b2)', '(on-table b1)', '(on-table b2)'})\n",
      "3\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b1)', '(clear b2)', '(on-table b1)', '(holding b3)', '(on-table b2)'})\n",
      "2\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "datasets/domain/blocks/train/blocks3-task02.pddl (6, 39) (6, 2)\n",
      "2\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(on-table b3)', '(on b1 b2)', '(clear b1)', '(handempty)', '(on-table b2)'})\n",
      "datasets/domain/blocks/train/blocks3-task03.pddl (7, 39) (7, 2)\n",
      "3\n",
      "frozenset({'(on b1 b3)', '(on-table b3)', '(on-table b2)'}) frozenset({'(on b2 b3)', '(on-table b3)', '(on b1 b2)', '(clear b1)', '(handempty)'})\n",
      "3\n",
      "frozenset({'(on b1 b3)', '(on-table b3)', '(on-table b2)'}) frozenset({'(on b2 b3)', '(clear b2)', '(holding b1)', '(on-table b3)'})\n",
      "2\n",
      "frozenset({'(on b1 b3)', '(on-table b3)', '(on-table b2)'}) frozenset({'(clear b1)', '(handempty)', '(on b2 b3)', '(clear b2)', '(on-table b1)', '(on-table b3)'})\n",
      "3\n",
      "frozenset({'(on b1 b3)', '(on-table b3)', '(on-table b2)'}) frozenset({'(clear b3)', '(clear b1)', '(holding b2)', '(on-table b1)', '(on-table b3)'})\n",
      "2\n",
      "frozenset({'(on b1 b3)', '(on-table b3)', '(on-table b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "datasets/domain/blocks/train/blocks3-task04.pddl (12, 39) (12, 2)\n",
      "3\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(on b2 b3)', '(on-table b3)', '(on b1 b2)', '(clear b1)', '(handempty)'})\n",
      "3\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(on b2 b3)', '(clear b2)', '(holding b1)', '(on-table b3)'})\n",
      "2\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b1)', '(handempty)', '(on b2 b3)', '(clear b2)', '(on-table b1)', '(on-table b3)'})\n",
      "3\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(holding b2)', '(on-table b1)', '(on-table b3)'})\n",
      "2\n",
      "frozenset({'(on-table b3)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "datasets/domain/blocks/train/blocks3-task05.pddl (17, 39) (17, 2)\n",
      "3\n",
      "frozenset({'(on-table b1)', '(on b3 b1)', '(on-table b2)'}) frozenset({'(on b2 b3)', '(on-table b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)'})\n",
      "3\n",
      "frozenset({'(on-table b1)', '(on b3 b1)', '(on-table b2)'}) frozenset({'(clear b3)', '(clear b1)', '(holding b2)', '(on-table b1)', '(on-table b3)'})\n",
      "2\n",
      "frozenset({'(on-table b1)', '(on b3 b1)', '(on-table b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "datasets/domain/blocks/train/blocks3-task06.pddl (20, 39) (20, 2)\n",
      "2\n",
      "frozenset({'(on b3 b2)', '(on-table b1)', '(on-table b2)'}) frozenset({'(clear b3)', '(on-table b3)', '(on b1 b2)', '(clear b1)', '(handempty)', '(on-table b2)'})\n",
      "3\n",
      "frozenset({'(on b3 b2)', '(on-table b1)', '(on-table b2)'}) frozenset({'(clear b3)', '(holding b1)', '(clear b2)', '(on-table b3)', '(on-table b2)'})\n",
      "2\n",
      "frozenset({'(on b3 b2)', '(on-table b1)', '(on-table b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "datasets/domain/blocks/train/blocks3-task07.pddl (23, 39) (23, 2)\n",
      "2\n",
      "frozenset({'(on b2 b1)', '(on-table b1)', '(on b3 b2)'}) frozenset({'(clear b3)', '(handempty)', '(on b3 b1)', '(clear b2)', '(on-table b1)', '(on-table b2)'})\n",
      "3\n",
      "frozenset({'(on b2 b1)', '(on-table b1)', '(on b3 b2)'}) frozenset({'(clear b1)', '(clear b2)', '(on-table b1)', '(holding b3)', '(on-table b2)'})\n",
      "2\n",
      "frozenset({'(on b2 b1)', '(on-table b1)', '(on b3 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "3\n",
      "frozenset({'(on b2 b1)', '(on-table b1)', '(on b3 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(holding b2)', '(on-table b1)', '(on-table b3)'})\n",
      "2\n",
      "frozenset({'(on b2 b1)', '(on-table b1)', '(on b3 b2)'}) frozenset({'(clear b3)', '(handempty)', '(on b2 b1)', '(clear b2)', '(on-table b1)', '(on-table b3)'})\n",
      "datasets/domain/blocks/train/blocks3-task08.pddl (28, 39) (28, 2)\n",
      "4\n",
      "frozenset({'(on b2 b3)', '(on-table b1)', '(on b3 b1)'}) frozenset({'(clear b3)', '(on b1 b2)', '(handempty)', '(on b3 b1)', '(on-table b2)'})\n",
      "4\n",
      "frozenset({'(on b2 b3)', '(on-table b1)', '(on b3 b1)'}) frozenset({'(clear b1)', '(holding b3)', '(on-table b2)', '(on b1 b2)'})\n",
      "3\n",
      "frozenset({'(on b2 b3)', '(on-table b1)', '(on b3 b1)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(on-table b2)', '(on-table b3)', '(on b1 b2)'})\n",
      "3\n",
      "frozenset({'(on b2 b3)', '(on-table b1)', '(on b3 b1)'}) frozenset({'(clear b3)', '(holding b1)', '(clear b2)', '(on-table b3)', '(on-table b2)'})\n",
      "2\n",
      "frozenset({'(on b2 b3)', '(on-table b1)', '(on b3 b1)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "3\n",
      "frozenset({'(on b2 b3)', '(on-table b1)', '(on b3 b1)'}) frozenset({'(clear b1)', '(clear b2)', '(on-table b1)', '(holding b3)', '(on-table b2)'})\n",
      "2\n",
      "frozenset({'(on b2 b3)', '(on-table b1)', '(on b3 b1)'}) frozenset({'(clear b3)', '(handempty)', '(on b3 b1)', '(clear b2)', '(on-table b1)', '(on-table b2)'})\n",
      "datasets/domain/blocks/train/blocks3-task09.pddl (35, 39) (35, 2)\n"
     ]
    }
   ],
   "source": [
    "generate_problem_matrix(\"datasets/domain/blocks/domain.pddl\", \n",
    "                        \"datasets/domain/blocks/train\", \n",
    "                        \"datasets/domain/blocks/plans\", \n",
    "                        \"datasets/results\", \n",
    "                        \"blocks-train-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28428429",
   "metadata": {},
   "source": [
    "# RankSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7fdf3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def transform_pairwise(X, y):\n",
    "    \"\"\"\n",
    "    Transforms data into pairs for convex relaxation of kendal rank correlation coef\n",
    "    In this method, all pairs are choosen, except for those that have the same target value or equal cost\n",
    "    Inputs\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The input feature vec of states from of several problems\n",
    "    y : array, shape (n_samples,) or (n_samples, 2)\n",
    "        The input cost vector. If it's a 2D array, the second column represents\n",
    "        the problem index\n",
    "    Returns\n",
    "    -------\n",
    "    X_trans : array, shape (k, n_feaures)\n",
    "        Difference between features of states (si - sj), only consider the state pair from the same problem\n",
    "    y_trans : array, shape (k,)\n",
    "        Output rank labels of values {-1, +1}, 1 represent si has potentially larger cost than sj (further away from goal)\n",
    "    \"\"\"\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    if y.ndim == 1:\n",
    "        y = np.c_[y, np.ones(y.shape[0])]\n",
    "    comb = itertools.combinations(range(X.shape[0]), 2)\n",
    "    for k, (i, j) in enumerate(comb):\n",
    "        if y[i, 0] == y[j, 0] or y[i, 1] != y[j, 1]:\n",
    "            # skip if they have the same cost or are from different problem group\n",
    "            continue\n",
    "        # otherwise, make the new pair-wise data\n",
    "        X_new.append(X[i] - X[j])\n",
    "        y_new.append(np.sign(y[i, 0] - y[j, 0])) # y = 1 if xi further away (larger cost), Vice Vesa\n",
    "        # randomly output some negative values for training purpose\n",
    "        if y_new[-1] != (-1) ** k:\n",
    "            y_new[-1] = - y_new[-1]\n",
    "            X_new[-1] = - X_new[-1]\n",
    "    return np.asarray(X_new), np.asarray(y_new)\n",
    "\n",
    "\n",
    "class RankSVM(svm.LinearSVC):\n",
    "    \"\"\"\n",
    "    Performs pairwise ranking svm with an underlying LinearSVC model\n",
    "    initialise with a C of regularization term\n",
    "    default using hinge loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C = 1.0):\n",
    "        super(RankSVM, self).__init__()\n",
    "        self.C = C\n",
    "        self.loss = 'hinge'\n",
    "        self.fit_intercept = False\n",
    "        self.max_iter = 9999\n",
    "#         self.scaler = StandardScaler()\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit a pairwise ranking model by first transfer it into pairwise than fitting\n",
    "        Inputs\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "        y : array, shape (n_samples,) or (n_samples, 2)\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        X_trans, y_trans = transform_pairwise(X, y)\n",
    "        super(RankSVM, self).fit(X_trans, y_trans)\n",
    "#         X_scaled = self.scaler.fit_transform(X_trans)\n",
    "#         super(RankSVM, self).fit(X_scaled, y_trans)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict an ordering on X. For a list of n samples, this method\n",
    "        returns a list from 0 to n-1 with the relative order of the rows of X.\n",
    "        Inputs\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        rtn: array, shape (n_samples,)\n",
    "            Returns a list of integers representing the relative order of\n",
    "            the rows in X.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'coef_'):\n",
    "#             return np.argsort(np.dot(self.scaler.transform(X), self.coef_.T).flatten())\n",
    "            return np.argsort(np.dot(X, self.coef_.T).flatten())\n",
    "        else:\n",
    "            raise ValueError(\"Must call fit() prior to predict()\")\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the accuracy for the rank prediction, from 0-1\n",
    "        \"\"\"\n",
    "        X_trans, y_trans = transform_pairwise(X, y)\n",
    "        return np.mean(super(RankSVM, self).predict(X_trans) == y_trans)\n",
    "    \n",
    "    def h_val(self, x):\n",
    "        \"\"\"\n",
    "        return pseduo heuristic for search\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'coef_'):\n",
    "            x_r = x.reshape(1,-1)\n",
    "#             return np.dot(self.scaler.transform(x_r), self.coef_.T)\n",
    "            return np.dot(x_r, self.coef_.T)\n",
    "        else:\n",
    "            raise ValueError(\"Must call fit() prior to predict()\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ae069",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "70c07092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankHeuristic(Heuristic):\n",
    "    \"\"\"\n",
    "    Implement the heuristic using the trained RankSVM's coef for dot product\n",
    "    \n",
    "    Inadmissible, directly reflect the rank\n",
    "    \n",
    "    Default scale value is 10000\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, svm : RankSVM, planning_graph, scale_val = 10000):\n",
    "        super().__init__()\n",
    "        self.svm = svm\n",
    "        self.planning_graph = planning_graph\n",
    "        self.scale = scale_val\n",
    "        \n",
    "    def __call__(self, node):\n",
    "        if (node.state == self.planning_graph.task.goals):\n",
    "            print(0)\n",
    "            return 0\n",
    "        vec = generate_feature_vec_relaxed(self.planning_graph, node.state, 9999999999)\n",
    "        h = round(self.svm.h_val(vec).item()*self.scale)\n",
    "        print(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69102e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ed0b30fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 39) (35, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RankSVM(C=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RankSVM</label><div class=\"sk-toggleable__content\"><pre>RankSVM(C=0.1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RankSVM(C=0.1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_train = RankSVM(C = 0.1)\n",
    "results = np.load(\"datasets/results/blocks-train-10.npz\")\n",
    "X = results['feature']\n",
    "Y = results['label']\n",
    "# print(X,Y)\n",
    "print(X.shape, Y.shape)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# print(X_scaled.shape, Y.shape)\n",
    "\n",
    "svm_train.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "49c264a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  0.  1.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  2.  0.  0.  0.  1.  1.  0.\n",
      "   3.  2.  3.]\n",
      " [17.  5.  6. 12.  6. 16.  2.  8.  0.  8. 12. 26.  6. 18.  0.  0.  2.  6.\n",
      "   0.  3.  8.  0.  3.  3.  0.  0.  8.  8. 14. 41.  3.  0.  1. 14.  5.  4.\n",
      "   5. 13.  5.]\n",
      " [18.  6.  7. 11.  6. 16.  2.  9.  0.  8. 13. 27.  9. 24.  0.  0.  3.  8.\n",
      "   0.  3.  9.  0.  4.  4.  0.  0.  9.  9. 13. 39.  3.  0.  1. 13.  4.  4.\n",
      "   5. 14.  4.]\n",
      " [15.  6.  5.  8.  6. 12.  2.  6.  0.  6. 10. 22.  9. 24.  0.  0.  3.  8.\n",
      "   0.  3.  6.  0.  2.  2.  0.  0.  6.  6.  5. 15.  2.  0.  0.  5.  0.  1.\n",
      "   4.  9.  5.]\n",
      " [14.  6.  5.  5.  8.  6.  4.  5.  0.  3. 11. 13.  9. 27.  0.  0.  3.  9.\n",
      "   0.  2.  4.  0.  2.  2.  0.  0.  5.  5.  6. 18.  2.  0.  0.  6.  0.  0.\n",
      "   4.  9.  4.]\n",
      " [ 9.  6.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  9. 27.  0.  0.  3.  9.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   3.  6.  5.]\n",
      " [ 6.  3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  3.  6.  0.  0.  1.  2.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  6.  1.  0.  0.  2.  0.  0.\n",
      "   3.  4.  4.]\n",
      " [ 8.  2.  3.  6.  2.  4.  3.  2.  0.  2.  5.  4.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  2.  0.  1.  1.  0.  0.  2.  2.  5. 14.  1.  0.  0.  5.  2.  1.\n",
      "   4.  5.  4.]\n",
      " [ 9.  3.  4.  5.  3.  4.  4.  3.  0.  2.  6.  5.  3.  6.  0.  0.  1.  2.\n",
      "   0.  0.  3.  0.  2.  2.  0.  0.  3.  3.  4. 12.  1.  0.  0.  4.  1.  1.\n",
      "   4.  5.  3.]\n",
      " [ 6.  3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  3.  6.  0.  0.  1.  2.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  6.  1.  0.  0.  2.  0.  0.\n",
      "   3.  4.  5.]] \n",
      " [[2. 0.]\n",
      " [6. 1.]\n",
      " [5. 1.]\n",
      " [4. 1.]\n",
      " [3. 1.]\n",
      " [2. 1.]\n",
      " [2. 2.]\n",
      " [6. 3.]\n",
      " [5. 3.]\n",
      " [4. 3.]]\n",
      "[4 3 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(X[:10],\"\\n\",Y[:10])\n",
    "\n",
    "print(svm_train.predict(X[1:6]))\n",
    "\n",
    "# svm_train.score(X[0:23], Y[0:23,0])\n",
    "\n",
    "# print(np.argsort(Y[23:46,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "503c4e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(on b3 b2)', '(on-table b1)', '(on-table b2)'})\n",
      "36280\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(holding b1)', '(on-table b2)', '(on b3 b2)'})\n",
      "44919\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b1)', '(clear b2)', '(on-table b1)', '(holding b3)', '(on-table b2)'})\n",
      "33801\n",
      "2\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "23801\n",
      "2\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(on b3 b2)', '(on-table b1)', '(on-table b2)'})\n",
      "36280\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(handempty)', '(on b3 b1)', '(clear b2)', '(on-table b1)', '(on-table b2)'})\n",
      "49185\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(holding b2)', '(on-table b1)', '(on-table b3)'})\n",
      "31758\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(holding b1)', '(clear b2)', '(on-table b3)', '(on-table b2)'})\n",
      "36351\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b1)', '(clear b2)', '(on-table b1)', '(holding b3)', '(on-table b2)'})\n",
      "33801\n",
      "2\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "23801\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(handempty)', '(on b2 b1)', '(clear b2)', '(on-table b1)', '(on-table b3)'})\n",
      "53821\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b1)', '(handempty)', '(on b2 b3)', '(clear b2)', '(on-table b1)', '(on-table b3)'})\n",
      "66545\n",
      "2\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(clear b2)', '(on-table b1)', '(on-table b3)', '(on-table b2)'})\n",
      "23801\n",
      "2\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(on b1 b2)', '(on-table b3)', '(on-table b2)'})\n",
      "29512\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b1)', '(handempty)', '(clear b2)', '(on b1 b3)', '(on-table b3)', '(on-table b2)'})\n",
      "59777\n",
      "1\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b1)', '(holding b3)', '(on-table b2)', '(on b1 b2)'})\n",
      "15616\n",
      "3\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(holding b1)', '(clear b2)', '(on-table b3)', '(on-table b2)'})\n",
      "36351\n",
      "2\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(clear b1)', '(handempty)', '(on-table b2)', '(on-table b3)', '(on b1 b2)'})\n",
      "29512\n",
      "0\n",
      "frozenset({'(on b3 b1)', '(on-table b2)', '(on b1 b2)'}) frozenset({'(clear b3)', '(handempty)', '(on-table b2)', '(on b3 b1)', '(on b1 b2)'})\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m rank_h \u001b[38;5;241m=\u001b[39m RankHeuristic(svm_train, test_plan)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# vec = generate_feature_vec_relaxed(test_plan, test_plan.task.goals, 9999999999)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(vec)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# h = svm_train.h_val(vec)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# print(h)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m gbfs(test_plan\u001b[38;5;241m.\u001b[39mtask, rank_h)\n",
      "File \u001b[1;32mE:\\@uni_life\\@COMP3770\\@project\\pyperplan\\pyperplan\\search\\a_star.py:94\u001b[0m, in \u001b[0;36mgreedy_best_first_search\u001b[1;34m(task, heuristic, use_relaxed_plan)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreedy_best_first_search\u001b[39m(task, heuristic, use_relaxed_plan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    Searches for a plan in the given task using greedy best first search.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m                     from a search node to reach the goal.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m astar_search(\n\u001b[0;32m     95\u001b[0m         task, heuristic, ordered_node_greedy_best_first, use_relaxed_plan\n\u001b[0;32m     96\u001b[0m     )\n",
      "File \u001b[1;32mE:\\@uni_life\\@COMP3770\\@project\\pyperplan\\pyperplan\\search\\a_star.py:180\u001b[0m, in \u001b[0;36mastar_search\u001b[1;34m(task, heuristic, make_open_entry, use_relaxed_plan)\u001b[0m\n\u001b[0;32m    177\u001b[0m         logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeeping operator \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    179\u001b[0m succ_node \u001b[38;5;241m=\u001b[39m searchspace\u001b[38;5;241m.\u001b[39mmake_child_node(pop_node, op, succ_state)\n\u001b[1;32m--> 180\u001b[0m h \u001b[38;5;241m=\u001b[39m heuristic(succ_node)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# don't bother with states that can't reach the goal anyway\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[99], line 20\u001b[0m, in \u001b[0;36mRankHeuristic.__call__\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 20\u001b[0m vec \u001b[38;5;241m=\u001b[39m generate_feature_vec_relaxed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplanning_graph, node\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;241m9999999999\u001b[39m)\n\u001b[0;32m     21\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvm\u001b[38;5;241m.\u001b[39mh_val(vec)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(h)\n",
      "Cell \u001b[1;32mIn[93], line 96\u001b[0m, in \u001b[0;36mgenerate_feature_vec_relaxed\u001b[1;34m(planning_graph, state, max_level)\u001b[0m\n\u001b[0;32m     93\u001b[0m eff_pos \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# add pre and eff into the empty dictionary for the first layer\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m act_layers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     97\u001b[0m     pre, eff_pos \u001b[38;5;241m=\u001b[39m append_to_dict(a, pre, eff_pos)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# loop through second to last action layers\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_plan = RelaxedPlanningGraph(\"datasets/domain/blocks/domain.pddl\", \n",
    "                                 \"datasets/domain/blocks/blocks/blocks3/task15.pddl\")\n",
    "\n",
    "rank_h = RankHeuristic(svm_train, test_plan)\n",
    "\n",
    "# vec = generate_feature_vec_relaxed(test_plan, test_plan.task.goals, 9999999999)\n",
    "# print(vec)\n",
    "# h = svm_train.h_val(vec)\n",
    "# print(h)\n",
    "\n",
    "gbfs(test_plan.task, rank_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9189ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0,1,2])\n",
    "print(a.shape)\n",
    "a = a.reshape(-1,1)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71358a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
